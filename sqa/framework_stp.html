<!DOCTYPE html><head><meta charset="UTF-8"><title>Framework Software Test Plan | Moltres</title><link href="../contrib/materialize/materialize.min.css" type="text/css" rel="stylesheet" media="screen,projection"></link><link href="../contrib/prism/prism.min.css" type="text/css" rel="stylesheet"></link><link href="../css/moose.css" type="text/css" rel="stylesheet"></link><link href="../css/devel_moose.css" type="text/css" rel="stylesheet"></link><link href="../css/alert_moose.css" type="text/css" rel="stylesheet"></link><link href="../css/content_moose.css" type="text/css" rel="stylesheet"></link><link href="../css/sqa_moose.css" type="text/css" rel="stylesheet"></link><link href="../css/civet_moose.css" type="text/css" rel="stylesheet"></link><link href="../css/moltres.css" type="text/css" rel="stylesheet"></link><script type="text/javascript" src="../contrib/jquery/jquery.min.js"></script></head><body><div class="page-wrap"><header><nav><div class="nav-wrapper container"><a href="../index.html" class="left moose-logo hide-on-med-and-down" id="home-button">Moltres</a><a href="https://github.com/arfc/moltres" class="right"><img src="../media/framework/github-logo.png" class="github-mark"></img><img src="../media/framework/github-mark.png" class="github-logo"></img></a><ul class="right hide-on-med-and-down"><li><a href="#!" class="dropdown-trigger" data-target="635b72d8-590c-4d7f-bc9f-d5fd11d7e68b" data-constrainWidth="false">Getting Started<i class="material-icons right">arrow_drop_down</i></a></li><li><a href="#!" class="dropdown-trigger" data-target="23653cc3-b8af-446c-b859-21bc8ee7512f" data-constrainWidth="false">Documentation<i class="material-icons right">arrow_drop_down</i></a></li><li><a href="#!" class="dropdown-trigger" data-target="581a36a4-a0fa-462c-a969-4a2869ff9c15" data-constrainWidth="false">Help<i class="material-icons right">arrow_drop_down</i></a></li><li><a href="../citing.html">Citing</a></li></ul><a href="#" class="sidenav-trigger" data-target="3a290255-82b2-4671-b489-a444bf2a74f5"><i class="material-icons">menu</i></a><ul class="sidenav" id="3a290255-82b2-4671-b489-a444bf2a74f5"><li><a href="#!" class="dropdown-trigger" data-target="396c89df-4964-4714-a0be-ee338672ad88" data-constrainWidth="false">Getting Started<i class="material-icons right">arrow_drop_down</i></a></li><li><a href="#!" class="dropdown-trigger" data-target="66f1dc0e-dff4-49d0-a0e2-1a177d59af55" data-constrainWidth="false">Documentation<i class="material-icons right">arrow_drop_down</i></a></li><li><a href="#!" class="dropdown-trigger" data-target="abb1187a-3c38-47c3-b5e0-2abaa146cf04" data-constrainWidth="false">Help<i class="material-icons right">arrow_drop_down</i></a></li><li><a href="../citing.html">Citing</a></li></ul><a href="#moose-search" class="modal-trigger"><i class="material-icons">search</i></a></div><ul class="dropdown-content" id="635b72d8-590c-4d7f-bc9f-d5fd11d7e68b"><li><a href="../getting_started/installation.html">Install Moltres</a></li><li><a href="../getting_started/tutorials.html">Tutorials</a></li></ul><ul class="dropdown-content" id="23653cc3-b8af-446c-b859-21bc8ee7512f"><li><a href="../syntax/index.html">Moltres Syntax</a></li><li><a href="https://mooseframework.inl.gov/source/index.html">MOOSE Syntax</a></li><li><a href="../doxygen/classes.html">Moltres Doxygen</a></li><li><a href="https://mooseframework.inl.gov/docs/doxygen/moose/classes.html">MOOSE Doxygen</a></li><li><a href="../development/contributing.html">Contributing</a></li><li><a href="../publications.html">List of Publications</a></li></ul><ul class="dropdown-content" id="581a36a4-a0fa-462c-a969-4a2869ff9c15"><li><a href="https://github.com/arfc/moltres/discussions">Moltres Discussion Forum</a></li><li><a href="https://github.com/idaholab/moose/discussions">MOOSE Discussion Forum</a></li></ul><ul class="dropdown-content" id="396c89df-4964-4714-a0be-ee338672ad88"><li><a href="../getting_started/installation.html">Install Moltres</a></li><li><a href="../getting_started/tutorials.html">Tutorials</a></li></ul><ul class="dropdown-content" id="66f1dc0e-dff4-49d0-a0e2-1a177d59af55"><li><a href="../syntax/index.html">Moltres Syntax</a></li><li><a href="https://mooseframework.inl.gov/source/index.html">MOOSE Syntax</a></li><li><a href="../doxygen/classes.html">Moltres Doxygen</a></li><li><a href="https://mooseframework.inl.gov/docs/doxygen/moose/classes.html">MOOSE Doxygen</a></li><li><a href="../development/contributing.html">Contributing</a></li><li><a href="../publications.html">List of Publications</a></li></ul><ul class="dropdown-content" id="abb1187a-3c38-47c3-b5e0-2abaa146cf04"><li><a href="https://github.com/arfc/moltres/discussions">Moltres Discussion Forum</a></li><li><a href="https://github.com/idaholab/moose/discussions">MOOSE Discussion Forum</a></li></ul></nav><div class="modal modal-fixed-footer moose-search-modal" id="moose-search"><div class="modal-content container moose-search-modal-content"><div class="row"><div class="col l12"><div class="input-field"><input type_="text" onkeyup="mooseSearch()" placeholder="/index.md" id="moose-search-box"></input></div></div><div><div class="col s12" id="moose-search-results"></div></div></div></div><div class="modal-footer"><a href="#!" class="modal-close btn-flat">Close</a></div></div></header><main class="main"><div class="container"><div class="row"><div class="col hide-on-med-and-down l12"><nav class="breadcrumb-nav"><div class="nav-wrapper"><span class="breadcrumb">sqa</span><a href="#" class="breadcrumb">framework_stp</a></div></nav></div></div><div class="row"><div class="moose-content col s12 m12 l10"><section id="e85df893-66be-480e-b000-2d62273acb2c" data-section-level="1" data-section-text="Framework Software Test Plan"><h1 id="framework-software-test-plan">Framework Software Test Plan</h1><p><em>This template follows <span class="tooltipped" data-tooltip="Idaho National Laboratory" data-position="top" data-delay="50">INL</span> template TEM-141, &quot;Software Test Plan.&quot;</em></p><section class="scrollspy" id="c8aeaa6f-f0e7-4ace-9635-167b5e40177d" data-section-level="2" data-section-text="Test Scope"><h2 id="test-scope">Test Scope</h2><p>This plan details the testing implemented for the development of MOOSE or MOOSE-based application that leads to a stable revision, which upon a secondary review by the Project Lead can be released. The <span class="tooltipped" data-tooltip="Nuclear Quality Assurance Level 1" data-position="top" data-delay="50">NQA-1</span> standard necessitates reviews and approvals for each release. This plan describes how automated testing fulfills obligatory testing of the software and how the Project Lead can leverage this information when performing a release.</p><p><span class="tooltipped" data-tooltip="Multiphysics Object Oriented Simulation Environment" data-position="top" data-delay="50">MOOSE</span> or MOOSE-based applications use an agile development method that tests all proposed changes prior to integration into the main repository. This allows developers to exercise proposed changes as they are written and ensure that existing code is not impacted in adverse ways. Testing is an integral part of the normal development process, as such the necessary testing and reviews to satisfy the <span class="tooltipped" data-tooltip="Nuclear Quality Assurance Level 1" data-position="top" data-delay="50">NQA-1</span> standard are natural for contributors to the project.</p><p>All testing performed is &quot;dynamic&quot; that attempt to identify defects by executing the software. All testing for each revision is performed automatically using <span>Continuous Integration for Verification, Enhancement, and Testing (CIVET)</span>. After automated testing has successfully completed and a technical review is performed, an automated merge is made into a &quot;stable&quot; revision. Each revision is eligible for release at the discretion of the Project Lead and subject to a complete release review. </p></section><section class="scrollspy" id="4e934aba-28b4-4c44-9d3d-b7af23866824" data-section-level="2" data-section-text="Test Objectives"><h2 id="test-objectives">Test Objectives</h2><p>All test types, as detailed in <a href="framework_stp.html#test-types">Types of Tests to be Executed</a>, for <span class="tooltipped" data-tooltip="Multiphysics Object Oriented Simulation Environment" data-position="top" data-delay="50">MOOSE</span> or MOOSE-based application have a single objective: that when executed with the prescribed input the software shall produce the expected output. The type of test indicates the type of output that shall be considered, which can range from numerical verification to error testing. The overall object is to provide the necessary confidence that the software will perform as expected for each for the defined test cases to ensure that the software properly handles abnormal conditions and events as well as credible failures, does not perform adverse unintended functions, and does not degrade the system either by itself, or in combination with other functions.</p><p>By the nature of the software as a library it is not possible to guarantee the functionality of the software from an end-user perspective, since the input provided by the user cannot be controlled. </p></section><section class="scrollspy" id="a80c9758-f789-4c0e-a14a-f1e3a3e51746" data-section-level="2" data-section-text="Assumptions"><h2 id="assumptions">Assumptions</h2><p><span class="tooltipped" data-tooltip="Multiphysics Object Oriented Simulation Environment" data-position="top" data-delay="50">MOOSE</span> and MOOSE-based applications are assumed to be dynamically-linked command-line UNIX (POSIX) compatible executable built on the target system. Being <span class="tooltipped" data-tooltip="High Performance Computing" data-position="top" data-delay="50">HPC</span> software, and the fact that our normal configuration relies on shared-libraries, it is generally not advisable to build <span class="tooltipped" data-tooltip="Multiphysics Object Oriented Simulation Environment" data-position="top" data-delay="50">MOOSE</span> on one system and execute it on another system (with the exception of a homogeneous cluster environment).</p><p><span class="tooltipped" data-tooltip="Multiphysics Object Oriented Simulation Environment" data-position="top" data-delay="50">MOOSE</span> and MOOSE-based applications are assumed to be stateless, reading all inputs from local or network mounted file-systems.  When deployed for parallel testing or use, standard <span class="tooltipped" data-tooltip="Message Passing Interface" data-position="top" data-delay="50">MPI</span> networking is expected to function among cluster compute nodes. <span class="tooltipped" data-tooltip="Multiphysics Object Oriented Simulation Environment" data-position="top" data-delay="50">MOOSE</span> or MOOSE-based applications does not require any special file system (i.e., parallel file systems), however high performance file systems can improve performance of large simulations and also the speed at which the automated testing system can launch, run, and inspect test results. </p></section><section class="scrollspy" id="caa1bfe0-4dc6-4894-8b90-1387794fd1ed" data-section-level="2" data-section-text="Constraints"><h2 id="constraints">Constraints</h2><p><span class="tooltipped" data-tooltip="Multiphysics Object Oriented Simulation Environment" data-position="top" data-delay="50">MOOSE</span> and MOOSE-based applications are designed to be built and executed in-situ on the end-use machine. There is no requirement for separate testing or acceptance environments, as each independent invocation of a simulation maintains its own environment. Acceptance testing may be performed at full-scale provided resources are available. Therefore, there are no constraints on testing of <span class="tooltipped" data-tooltip="Multiphysics Object Oriented Simulation Environment" data-position="top" data-delay="50">MOOSE</span> and MOOSE-based applications.</p></section><section class="scrollspy" id="6669fcda-2ccb-470e-9943-24fb5b765e6b" data-section-level="2" data-section-text="Types of Tests to be Executed"><h2 id="test-types">Types of Tests to be Executed</h2><p>It is possible to categorize test cases in many ways such as &quot;system&quot;, &quot;integration&quot;, &quot;performance&quot;, or &quot;acceptance&quot; testing. <span class="tooltipped" data-tooltip="Multiphysics Object Oriented Simulation Environment" data-position="top" data-delay="50">MOOSE</span> and MOOSE-based applications do not categorize test cases; they are simply defined and executed as a complete set and automatically executed as defined in <a href="framework_stp.html#test-automation">Test Automation (Scripting)</a>.</p><section id="34ce57dd-df9c-42c9-a002-fcc2de757178" data-section-level="3" data-section-text="Required tests and test sequence"><h3 id="required-tests-and-test-sequence">Required tests and test sequence</h3><p>All tests defined within <span class="tooltipped" data-tooltip="Multiphysics Object Oriented Simulation Environment" data-position="top" data-delay="50">MOOSE</span> or MOOSE-based application must be executed and pass for all revisions and releases of the software. All test cases within one test specification (a &quot;tests&quot; file) are expected to run in the order defined. The specifications may be executed in any order.</p></section><section id="63cd472f-35f4-4a05-8582-72fa9c01324c" data-section-level="3" data-section-text="Required Ranges of input parameters"><h3 id="required-ranges-of-input-parameters">Required Ranges of input parameters</h3><p>Test cases are created by contributors during the change control process. The independent reviewer is responsible for ensuring that input parameters are tested across the expected operational ranges to the extent necessary for the proposed change.</p></section><section id="5f7a51b7-197e-4749-98a1-736cb45929e7" data-section-level="3" data-section-text="Identification of the stages at which testing is required"><h3 id="identification-of-the-stages-at-which-testing-is-required">Identification of the stages at which testing is required</h3><p>Testing for <span class="tooltipped" data-tooltip="Multiphysics Object Oriented Simulation Environment" data-position="top" data-delay="50">MOOSE</span> and MOOSE-based applications shall include the levels of testing as defined in <a href="#fig:civet_flow">Figure 1</a>. The testing is automated to the extent possible and the &quot;Next&quot; and &quot;Devel&quot; branch testing may be combined at the discretion of the application.</p><div class="card moose-float" id="fig:civet_flow"><div class="card-content"><picture class="materialboxed moose-image"><img src="../large_media/tutorials/darcy_thermo_mech/civet_flow.png"></img></picture><p class="moose-caption"><span class="moose-caption-heading">Figure 1: </span><span class="moose-caption-text" id="fig:civet_flow">Required stages for testing of <span class="tooltipped" data-tooltip="Multiphysics Object Oriented Simulation Environment" data-position="top" data-delay="50">MOOSE</span>, MOOSE-based                          applications, and . The &quot;Next&quot; and &quot;Devel&quot; branch testing may be combined                          at the discretion of the application.</span></p></div></div></section><section id="7feb40fa-6f0e-4354-9a0a-8d6df90417c5" data-section-level="3" data-section-text="Requirement for testing logic branches"><h3 id="requirement-for-testing-logic-branches">Requirement for testing logic branches</h3><p>Test cases are created by contributors during the change control process. The independent reviewer is responsible for ensuring that all logical code paths are tested to the extent necessary for the proposed change.</p></section><section id="b2f4f1bc-32da-4054-8841-553efb0c2b56" data-section-level="3" data-section-text="Requirements for hardware integration"><h3 id="requirements-for-hardware-integration">Requirements for hardware integration</h3><p>The hardware and software configurations tested for <span class="tooltipped" data-tooltip="Multiphysics Object Oriented Simulation Environment" data-position="top" data-delay="50">MOOSE</span> and MOOSE-based applications are at the discretion of the Project Lead. Upon release the hardware and software configurations utilized shall be included in the release.</p></section><section id="81fcea9b-e929-4293-8203-4ebc6506d050" data-section-level="3" data-section-text="Anticipated output values"><h3 id="anticipated-output-values">Anticipated output values</h3><p>The anticipated output values of each test cases are defined within each test case. The format of the output is dictated by the &quot;Tester&quot; as detailed in <a href="framework_stp.html#test-automation">Test Automation (Scripting)</a>.</p></section><section id="d04b99ee-8c6b-46c7-ba76-80ad304a53d7" data-section-level="3" data-section-text="Acceptance criteria"><h3 id="acceptance-criteria">Acceptance criteria</h3><p>All tests have a pass/fail acceptance criteria based on the anticipated output as dictated by the &quot;Tester&quot;, see <a href="framework_stp.html#test-automation">Test Automation (Scripting)</a>. If the execution output matches the anticipated output than the test is accepted (pass), otherwise it is rejected (fail).</p><p>In addition, test coverage reports will be created for all proposed changes. Ideally, the overall line coverage should increase or remain constant at the discretion of the reviewer, but coverage should never drop below the level of 80%. Additionally, the independent reviewer is expected to use the coverage reports to ensure that the proposed changes are tested at an appropriate level.</p></section><section id="e24f359b-70d2-4164-a2bb-83f2120819bb" data-section-level="3" data-section-text="Reports , records , standard formatting , and conventions"><h3 id="reports-records-standard-formatting-and-conventions">Reports, records, standard formatting, and conventions</h3><p>Each test case shall report the acceptance status (pass or fail). If the test case fails the reason for the failure shall be included. The information reported is dependent on the &quot;Tester&quot;, see <a href="framework_stp.html#test-automation">Test Automation (Scripting)</a> for details. </p></section></section><section class="scrollspy" id="d5383db9-f3d4-4a21-977d-8ca415610913" data-section-level="2" data-section-text="Approval Requirements"><h2 id="approval-requirements">Approval Requirements</h2><p> All test cases are created by contributors during the development process and approved by independent reviewer. The creation of the test cases follows the change control process. These test cases can be in response to bug fixes or as a part of an enhancement. </p></section><section class="scrollspy" id="737f31ca-21cc-4107-bfaa-7afbedad315e" data-section-level="2" data-section-text="Test Iteration"><h2 id="test-iteration">Test Iteration</h2><p><span class="tooltipped" data-tooltip="Multiphysics Object Oriented Simulation Environment" data-position="top" data-delay="50">MOOSE</span> and MOOSE-based applications are stateless, deterministic software for a given input. Therefore, a single testing iteration on each identified configuration is sufficient for completing the required tests necessary for deployment.</p></section><section class="scrollspy" id="96e46a93-0320-4e1a-b462-a66a0319667c" data-section-level="2" data-section-text="Test Automation ( Scripting )"><h2 id="test-automation">Test Automation (Scripting)</h2><p><span class="tooltipped" data-tooltip="Multiphysics Object Oriented Simulation Environment" data-position="top" data-delay="50">MOOSE</span> and MOOSE-based applications rely heavily on full test automation. Since each application is stateless and command-line driven, developing a thorough test suite is generally more straightforward than it is for other business system type software. The MOOSE repository includes a general-purpose, extendable &quot;Test Harness&quot;, which is heavily leveraged to run the test cases. The Test Harness is used throughout all phases of MOOSE development from initial development, the change request process, deployment testing, and finally end-use in-situ testing. The Test Harness is even suitable for testing on large deployment clusters and supports the &quot;PBS&quot; queuing system.</p><p>The Test Harness includes a suite of &quot;Tester&quot; types to enable complete testing of <span class="tooltipped" data-tooltip="Multiphysics Object Oriented Simulation Environment" data-position="top" data-delay="50">MOOSE</span> and MOOSE-based applications. For each of the types, the Test Harness is able to execute the application with a developer designed input and verify the correct result automatically.  A complete list of the built-in Testers is included here:</p><p></p><p><strong>RunApp</strong><br>A tester designed to assemble common command line arguments for executing MOOSE-based application including launching with MPI or threads.<br><em>Anticipated Output</em>: A return code of 0 or 1.<br><em>Acceptance Criteria</em>: A non-zero return code is acceptable (pass).<br><em>Reports and Records</em>: The return code and the system standard output returned from the application.</p><p><strong>RunCommand</strong><br>A generic tester that can execute an arbitrary command.<br><em>Anticipated Output</em>: A return code of 0 or 1.<br><em>Acceptance Criteria</em>: A non-zero return code is acceptable (pass).<br><em>Reports and Records</em>: The return code and the system standard output returned from the command.</p><p><strong>FileTester</strong><br>An intermediate base class that runs a MOOSE-based command and expects to process a file written out by the MOOSE-based simulation.<br><em>Anticipated Output</em>: Files created as a result of the execution of an application.<br><em>Acceptance Criteria</em>: If the files are created the result is acceptable (pass).<br><em>Reports and Records</em>: This list of files missing upon failure</p><p><strong>RunException</strong><br>A tester that expects the application to produce a warning or error based on bad inputs, missing files, permissions, etc.<br><em>Anticipated Output</em>: A non-zero exist status and an error message.<br><em>Acceptance Criteria</em>: If the exit status is non-zero and the error message is present the result is acceptable (pass).<br><em>Reports and Records</em>: The return code and the system standard output returned from the application.</p><p><strong>CheckFiles</strong><br>A tester that looks for the creation of specific files after a simulation runs without regard for the contents of those files.<br><em>Anticipated Output</em>: Files created with the expected content.<br><em>Acceptance Criteria</em>: If the files are created the desired content exists then result is acceptable (pass).<br><em>Reports and Records</em>: This list of files created an the missing content.</p><p><strong>Exodiff</strong><br>Compares &quot;ExodusII&quot; format files output by the simulation to those checked into the repository as the &quot;gold&quot; standard for a given test within numeric tolerances.<br><em>Anticipated Output</em>: ExodusII files created as a result of the execution of an application.<br><em>Acceptance Criteria</em>: If the files created match a known &quot;gold&quot; standard the result is acceptable (pass).<br><em>Reports and Records</em>: Upon failure, a report detailing the differences in the files.</p><p><strong>CSVDiff</strong>:<br>Compares &quot;CSV&quot; format files output by the simulation to those checked into the repository as the &quot;gold&quot; standard for a given test within numeric tolerances.<br><em>Anticipated Output</em>: CSV files created as a result of the execution of an application.<br><em>Acceptance Criteria</em>: If the files created match a known &quot;gold&quot; standard the result is acceptable (pass).<br><em>Reports and Records</em>: Upon failure, a report detailing the differences in the files.</p><p><strong>ImageDiff</strong><br>Compares various image format files output by the simulation to those checked into the repository as the &quot;gold&quot; standard for a given test within tolerance.<br><em>Anticipated Output</em>: Image files created as a result of the execution of an application.<br><em>Acceptance Criteria</em>: If the files created match a known &quot;gold&quot; standard the result is acceptable (pass).<br><em>Reports and Records</em>: Upon failure, a report detailing the differences in the files.</p><p><strong>VTKDiff</strong><br>Compares &quot;VTK&quot; format files output by the simulation to those checked into the repository as the &quot;gold&quot; standard for a given test within numeric tolerances.<br><em>Anticipated Output</em>: VTK files created as a result of the execution of an application.<br><em>Acceptance Criteria</em>: If the files created match a known &quot;gold&quot; standard the result is acceptable (pass).<br><em>Reports and Records</em>: Upon failure, a report detailing the differences in the files.</p><p><strong>JacobianTester</strong><br>Appends additional arguments to the command line to trigger special solver modes for the purpose of producing &quot;finite-difference&quot; Jacobians for which to compare the Jacobians produces by the simulation.<br><em>Anticipated Output</em>: A return code of 0 or 1.<br><em>Acceptance Criteria</em>: If the simulation values match the finite-difference the result is acceptable (pass).<br><em>Reports and Records</em>: Upon failure, a report detailing the differences between the Jacobians.</p><p><strong>PythonUnitTest</strong><br>A tested designed to run Python scripts within the MOOSE test suite, generally containing unit tests.<br><em>Anticipated Output</em>: A script return code of 0 or 1.<br><em>Acceptance Criteria</em>: A 0 return code (meaning the script completed successfully) is acceptable (pass).<br><em>Reports and Records</em>: Upon failure, a report detailing the failures experienced within the test script.</p><p> </p></section><section class="scrollspy" id="7c0aadcd-12e2-4495-b797-39a36fb28df7" data-section-level="2" data-section-text="Resource Requirement"><h2 id="resource-requirement">Resource Requirement</h2><p>This section must clearly articulate what type of skill set is required for all the planned testing and when and for how long the resources are required.</p><p>In this section, the test planner must also identify the testing environment requirements, such as storage, servers, number of licenses for test automation tools...</p><p></p><section id="f06a72ea-07db-471f-a18a-873efe231678" data-section-level="3" data-section-text="Human Resources"><h3 id="human-resources">Human Resources</h3><p>Testing for <span class="tooltipped" data-tooltip="Multiphysics Object Oriented Simulation Environment" data-position="top" data-delay="50">MOOSE</span> and MOOSE-based applications requires minimal human resources. A system engineer is required to ensure the proper end-user environment is setup with proper system prerequisites. The Project Lead is should verify that the automated test system operated correctly prior to release.</p></section><section id="94d2979d-2795-4b40-8af3-0131ae62b6dc" data-section-level="3" data-section-text="Hardware / Software Resources"><h3 id="hardware-software-resources">Hardware/Software Resources</h3><p>If a specific end-user environment is required by a customer, those specifications must be supplied to the system engineer to prepare that environment. Alternatively, if remote access is available to the end-user system, the system engineer may be granted proper permissions to assist in setting up the environment on the customer&#x27;s system.</p><p>If no specific customer is required for a specific release, <span class="tooltipped" data-tooltip="Multiphysics Object Oriented Simulation Environment" data-position="top" data-delay="50">MOOSE</span> and MOOSE-based applications will be tested under the standard supported build system configuration(s). These systems are generally modern Linux and macOS distributions with recent compilers. Specific information on the tested environments for a release is stored in the release. </p></section><section id="bec23cfe-058e-4387-8a63-3835a7d4a099" data-section-level="3" data-section-text="Services / Applications"><h3 id="services-applications">Services/Applications</h3><p><span class="tooltipped" data-tooltip="Multiphysics Object Oriented Simulation Environment" data-position="top" data-delay="50">MOOSE</span> and MOOSE-based applications generally does not require any additional resources beyond the end-use system once the software is installed. During installation either an Internet connection or media containing the software must be available to install the software. Internet connectivity is not required after installation on the end-use system.</p></section></section><section class="scrollspy" id="e1ef9c33-8df1-4af0-93d2-aef8145d4ae5" data-section-level="2" data-section-text="Tasks and Responsibilities"><h2 id="tasks-and-responsibilities">Tasks and Responsibilities</h2><p> The creation and execution of the test cases is part of the change control process, as such the associated roles and reponsibilties are minimal.</p><div form="['left', 'left', 'left']" recursive class="moose-table-div"><table><thead><tr><th style=";text-align:left">Task</th><th style=";text-align:left">Responsibility</th><th style=";text-align:left">Role</th></tr></thead><tbody><tr><td style=";text-align:left">1.</td><td style=";text-align:left">Complete programming and test case(s)</td><td style=";text-align:left">Contributor</td></tr></tbody><tbody><tr><td style=";text-align:left">2.</td><td style=";text-align:left">Review test cases and automated results</td><td style=";text-align:left">Independent reviewer</td></tr></tbody><tbody><tr><td style=";text-align:left">3.</td><td style=";text-align:left">Review and approve final results for release</td><td style=";text-align:left">Project lead</td></tr></tbody></table></div><p> </p></section></section></div><div class="col hide-on-med-and-down l2"><div class="toc-wrapper pin-top"><ul class="section table-of-contents"><li><a href="#c8aeaa6f-f0e7-4ace-9635-167b5e40177d" class="tooltipped" data-position="left" data-tooltip="Test Scope">Test Scope</a></li><li><a href="#4e934aba-28b4-4c44-9d3d-b7af23866824" class="tooltipped" data-position="left" data-tooltip="Test Objectives">Test Objectives</a></li><li><a href="#a80c9758-f789-4c0e-a14a-f1e3a3e51746" class="tooltipped" data-position="left" data-tooltip="Assumptions">Assumptions</a></li><li><a href="#caa1bfe0-4dc6-4894-8b90-1387794fd1ed" class="tooltipped" data-position="left" data-tooltip="Constraints">Constraints</a></li><li><a href="#6669fcda-2ccb-470e-9943-24fb5b765e6b" class="tooltipped" data-position="left" data-tooltip="Types of Tests to be Executed">Types of Tests to be Executed</a></li><li><a href="#d5383db9-f3d4-4a21-977d-8ca415610913" class="tooltipped" data-position="left" data-tooltip="Approval Requirements">Approval Requirements</a></li><li><a href="#737f31ca-21cc-4107-bfaa-7afbedad315e" class="tooltipped" data-position="left" data-tooltip="Test Iteration">Test Iteration</a></li><li><a href="#96e46a93-0320-4e1a-b462-a66a0319667c" class="tooltipped" data-position="left" data-tooltip="Test Automation ( Scripting )">Test Automation ( Scripting )</a></li><li><a href="#7c0aadcd-12e2-4495-b797-39a36fb28df7" class="tooltipped" data-position="left" data-tooltip="Resource Requirement">Resource Requirement</a></li><li><a href="#e1ef9c33-8df1-4af0-93d2-aef8145d4ae5" class="tooltipped" data-position="left" data-tooltip="Tasks and Responsibilities">Tasks and Responsibilities</a></li></ul></div></div></div></div></main></div></body><script type="text/javascript" src="../contrib/materialize/materialize.min.js"></script><script type="text/javascript" src="../contrib/clipboard/clipboard.min.js"></script><script type="text/javascript" src="../contrib/prism/prism.min.js"></script><script type="text/javascript" src="../js/init.js"></script><script type="text/javascript" src="../js/navigation.js"></script><script type="text/javascript" src="../contrib/fuse/fuse.min.js"></script><script type="text/javascript" src="../js/search_index.js"></script><script type="text/javascript" src="../js/sqa_moose.js"></script>